[training]
gold_preproc = false
# Limitations on training document length or number of examples.
max_length = 500
limit = 0
patience = 10000
eval_frequency = 400
dropout = 0.1
init_tok2vec = null
max_epochs = 0
max_steps = 0
orth_variant_level = 0.0
seed = 0

scores = ["speed", "cats_macro_f"]
score_weights = {"cats_macro_f": 1.0}

base_model = null
use_pytorch_for_gpu_memory = false
raw_text = null
tag_map = null
vectors = null
morph_rules = null

batch_by = "padded"
batch_size = 2000
accumulate_gradient = 3
discard_oversize = true
eval_batch_size = 256

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
eps = 1e-8
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false

[training.optimizer.learn_rate]
@schedules = "warmup_linear.v1"
warmup_steps = 250
total_steps = 20000
initial_rate = 5e-5

[components]

[components.transformer]
factory = "transformer"
max_batch_items = 4096

# This loads the Huggingface Transformers model. The transformer is applied
# to a batch of Doc objects, which are preprocessed into Span objects to support
# longer documents.
[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v1"
name = "roberta-base"
tokenizer_config = {"use_fast": true}

[components.transformer.model.get_spans]
# You can set a custom strategy for preparing spans from the batch, e.g. you
# can predict over sentences. Here we predict over the whole document.
@span_getters = "strided_spans.v1"
window = 128
stride = 96

[components.textcat]
factory = "textcat"

[components.textcat.model]
@architectures = "spacy.TextCatCNN.v1"
exclusive_classes = false

[components.textcat.model.tok2vec]
@architectures = "spacy-transformers.Tok2VecListener.v1"
grad_factor = 1.0

[components.textcat.model.tok2vec.pooling]
@layers = "reduce_mean.v1"

[nlp]
lang = "en"
pipeline = ["transformer", "textcat"]
load_vocab_data = true
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[pretraining]
max_epochs = 1000
min_length = 5
max_length = 500
dropout = 0.2
n_save_every = null
batch_size = 3000
seed = 0
use_pytorch_for_gpu_memory = false
tok2vec_model = "components.tok2vec.model"

[pretraining.objective]
type = "characters"
n_characters = 4

[pretraining.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = true
eps = 0.00000001
learn_rate = 0.001
